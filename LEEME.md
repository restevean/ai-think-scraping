# ThinkScraper

Web scraper para recopilar opiniones, mensajes y conversaciones de m√∫ltiples plataformas sobre desarrollo de software.

## Requisitos

- Python 3.12+
- UV para gesti√≥n de entornos y dependencias

## Instalaci√≥n

```bash
# Clonar el repositorio
git clone <repo>
cd ai-think-scrapping

# Crear entorno virtual con UV
uv venv

# Activar entorno
source .venv/bin/activate  # En Linux/Mac
.venv\Scripts\activate     # En Windows

# Instalar dependencias
uv pip install -e .

# Instalar dependencias de desarrollo (opcional)
uv pip install -e ".[dev]"
```

## Quick Start

```bash
# Listar plataformas soportadas
thinkscraper list-platforms

# Scrapear una URL
thinkscraper scrape-url https://reddit.com/r/Python

# Scrapear m√∫ltiples URLs
thinkscraper scrape-urls urls.txt --output resultados.json

# Ver resumen de resultados
thinkscraper show-summary resultados.json

# Exportar a CSV
thinkscraper export-results resultados.json reporte.csv --format csv
```

## Plataformas Soportadas

- Reddit
- Stack Overflow
- Medium
- Dev.to

## Estructura del Proyecto

```
ai-think-scrapping/
‚îú‚îÄ‚îÄ src/
‚îÇ   ‚îú‚îÄ‚îÄ abstractions.py      # Interfaces (IScraper, IHttpClient, etc.)
‚îÇ   ‚îú‚îÄ‚îÄ base_scraper.py      # Template Method pattern
‚îÇ   ‚îú‚îÄ‚îÄ cli.py               # Click CLI interface
‚îÇ   ‚îú‚îÄ‚îÄ config.py            # Configuraci√≥n centralizada
‚îÇ   ‚îú‚îÄ‚îÄ http_client.py       # Cliente HTTP con reintentos
‚îÇ   ‚îú‚îÄ‚îÄ json_storage.py      # Almacenamiento en JSON
‚îÇ   ‚îú‚îÄ‚îÄ models.py            # Pydantic models (Message, ScrapingResult)
‚îÇ   ‚îú‚îÄ‚îÄ orchestrator.py      # Coordinador de scrapers
‚îÇ   ‚îú‚îÄ‚îÄ parsers.py           # Parsers espec√≠ficos por plataforma
‚îÇ   ‚îú‚îÄ‚îÄ scraper_factory.py   # Factory Pattern
‚îÇ   ‚îî‚îÄ‚îÄ scrapers.py          # Scrapers para cada plataforma
‚îú‚îÄ‚îÄ tests/
‚îÇ   ‚îú‚îÄ‚îÄ test_abstractions.py
‚îÇ   ‚îú‚îÄ‚îÄ test_cli.py
‚îÇ   ‚îú‚îÄ‚îÄ test_implementations.py
‚îÇ   ‚îú‚îÄ‚îÄ test_models.py
‚îÇ   ‚îú‚îÄ‚îÄ test_orchestrator.py
‚îÇ   ‚îî‚îÄ‚îÄ test_scrapers.py
‚îú‚îÄ‚îÄ docs/
‚îÇ   ‚îú‚îÄ‚îÄ ARCHITECTURE.md      # Arquitectura t√©cnica y dise√±o
‚îÇ   ‚îú‚îÄ‚îÄ EXAMPLES.md          # Casos de uso y ejemplos
‚îÇ   ‚îî‚îÄ‚îÄ README.md            # √çndice de documentaci√≥n
‚îî‚îÄ‚îÄ data/                    # Resultados en JSON
```

## Ejecuci√≥n de Tests

```bash
# Ejecutar todos los tests
pytest

# Con reporte de cobertura
pytest --cov=src --cov-report=html

# Tests espec√≠ficos
pytest tests/test_cli.py -v
```

## Caracter√≠sticas

‚úÖ Scraping de m√∫ltiples plataformas
‚úÖ CLI profesional con Click
‚úÖ Arquitectura SOLID
‚úÖ Tests completos con TDD
‚úÖ Manejo de errores robusto
‚úÖ Rate limiting y reintentos autom√°ticos
‚úÖ Exportaci√≥n a JSON y CSV
‚úÖ Type hints y validaci√≥n con Pydantic
‚úÖ Cobertura de c√≥digo >95%

## Documentaci√≥n

- [ARCHITECTURE.md](./docs/ARCHITECTURE.md) - Arquitectura t√©cnica, patrones de dise√±o y SOLID
- [EXAMPLES.md](./docs/EXAMPLES.md) - Casos de uso, ejemplos CLI y program√°ticos
- [docs/README.md](./docs/README.md) - √çndice de documentaci√≥n

## Licencia

Este proyecto est√° licenciado bajo la licencia MIT - ver el archivo [LICENSE](./LICENSE) para m√°s detalles.

# Explicaci√≥n Detallada del Comando ThinkScraper

## Nivel 1: Estructura General

ThinkScraper es una **herramienta CLI (Command Line Interface)** que funciona con la estructura:

```bash
thinkscraper [COMANDO] [ARGUMENTOS] [OPCIONES]
```

Tiene **7 comandos principales**.

---

## Nivel 2: Comandos Disponibles

### 1Ô∏è‚É£ `list-platforms` (El m√°s simple)

```bash
thinkscraper list-platforms
```

**Qu√© hace:** Muestra las plataformas que ThinkScraper soporta.

**Salida:**
```
üìã Supported Platforms:
  ‚Ä¢ devto
  ‚Ä¢ medium
  ‚Ä¢ reddit
  ‚Ä¢ stackoverflow
```

---

### 2Ô∏è‚É£ `scrape-url` (Una URL, simple)

```bash
thinkscraper scrape-url "https://reddit.com/r/Python"
```

**Qu√© hace:** Scrappea una √∫nica URL detectando autom√°ticamente su plataforma.

**Con opci√≥n output:**
```bash
thinkscraper scrape-url "https://reddit.com/r/Python" --output resultado.json
```

**Salida:**
```
üîç Scraping: https://reddit.com/r/Python
‚úÖ Success! Extracted 15 messages
üìÅ Results saved to: resultado.json
```

---

### 3Ô∏è‚É£ `scrape-urls` (M√∫ltiples URLs desde archivo)

Primero creas un archivo de texto:

```bash
cat > urls.txt << EOF
https://reddit.com/r/Python
https://stackoverflow.com/questions/123
https://medium.com/@author/article
EOF
```

Luego lo scrapeas:

```bash
thinkscraper scrape-urls urls.txt --output resultados.json
```

**Qu√© hace:**
- Lee TODAS las URLs del archivo `urls.txt` (una por l√≠nea)
- Scrappea cada una detectando su plataforma autom√°ticamente
- Guarda resultados combinados en `resultados.json`
- Genera un resumen estad√≠stico

**Salida:**
```
üîç Scraping 3 URLs from urls.txt
‚úÖ Completed: 3/3 successful
   Total messages extracted: 120
   Success rate: 100.0%
üìÅ Results saved to: resultados.json
```

**Opci√≥n importante:**
```bash
thinkscraper scrape-urls urls.txt --output resultados.json --skip-errors
```

La opci√≥n `--skip-errors` hace que contin√∫e aunque algunas URLs fallen. Sin ella, si una URL falla, se detiene TODO.

---

### 4Ô∏è‚É£ `scrape-platform` (M√∫ltiples URLs de UNA plataforma)

```bash
thinkscraper scrape-platform reddit \
  https://reddit.com/r/Python \
  https://reddit.com/r/JavaScript \
  https://reddit.com/golang
```

**Qu√© hace:**
- Scrappea m√∫ltiples URLs de la MISMA plataforma
- Es m√°s eficiente que `scrape-urls` porque **no necesita detectar plataforma para cada URL**
- Usa el mismo scraper para todas (m√°s r√°pido)

**Ventaja de rendimiento:**
```
scrape-urls     ‚Üí Detecta plataforma 3 veces (lento)
scrape-platform ‚Üí Usa el mismo scraper 3 veces (r√°pido)
```

---

### 5Ô∏è‚É£ `show-summary` (Ver resumen de resultados)

```bash
thinkscraper show-summary resultados.json
```

**Qu√© hace:** Muestra un resumen de los datos extra√≠dos sin abrir el archivo JSON.

**Salida:**
```
üìä Scraping Summary:

  Total URLs:        5
  Successful:        4
  Failed:            1
  Total Messages:    245
  Success Rate:      80.0%
```

---

### 6Ô∏è‚É£ `export-results` (Convertir JSON a otro formato)

```bash
thinkscraper export-results resultados.json reporte.csv --format csv
```

**Qu√© hace:** Convierte los resultados JSON a CSV (o mantiene JSON).

**Formatos disponibles:**
- `--format json` ‚Üí Mantiene formato JSON
- `--format csv` ‚Üí Convierte a CSV

**CSV resultante:**
```csv
url,success,messages_count,error
https://reddit.com/r/Python,True,15,
https://stackoverflow.com/q/123,True,8,
https://invalid.com,False,0,No scraper supports URL
```

---

## Nivel 3: Estructura del JSON de Salida

Cuando haces scraping y usas `--output`, obtienes algo como:

```json
{
  "results": [
    {
      "success": true,
      "url": "https://reddit.com/r/Python",
      "messages_count": 15,
      "error": null,
      "timestamp": "2024-11-04T10:30:00"
    },
    {
      "success": false,
      "url": "https://invalid.com",
      "messages_count": 0,
      "error": "No scraper supports URL",
      "timestamp": "2024-11-04T10:31:00"
    }
  ],
  "summary": {
    "total_urls": 2,
    "successful": 1,
    "failed": 1,
    "total_messages": 15,
    "success_rate": 50.0
  }
}
```

---

## Nivel 4: Flujo Completo (C√≥mo usarlos juntos)

**Paso 1:** Crear archivo con URLs
```bash
cat > mis_urls.txt << EOF
https://reddit.com/r/Python
https://stackoverflow.com/questions/12345
https://medium.com/@author/article
EOF
```

**Paso 2:** Scrapear todas
```bash
thinkscraper scrape-urls mis_urls.txt --output resultados.json
```

**Paso 3:** Ver resumen
```bash
thinkscraper show-summary resultados.json
```

**Paso 4:** Exportar a CSV para an√°lisis
```bash
thinkscraper export-results resultados.json reporte.csv --format csv
```

**Resultado final:** `reporte.csv` listo para abrir en Excel.

---

## Nivel 5: Opciones Disponibles por Comando

### `thinkscraper scrape-url`
- `--output, -o` ‚Üí Archivo de salida (JSON)
- `--help` ‚Üí Mostrar ayuda

### `thinkscraper scrape-urls`
- `--output, -o` ‚Üí Archivo de salida (JSON) [REQUERIDO]
- `--skip-errors` ‚Üí Continuar si hay errores
- `--help` ‚Üí Mostrar ayuda

### `thinkscraper scrape-platform`
- `--output, -o` ‚Üí Archivo de salida (JSON)
- `--help` ‚Üí Mostrar ayuda

### `thinkscraper export-results`
- `--format` ‚Üí Formato de salida (json, csv) [default: json]
- `--help` ‚Üí Mostrar ayuda

---

## Resumen Visual de Todos los Comandos

| Comando | Para qu√© | Entrada | Salida |
|---------|----------|---------|--------|
| `list-platforms` | Ver plataformas soportadas | Ninguna | Terminal |
| `scrape-url` | Scrapear 1 URL | URL en terminal | JSON (opcional) |
| `scrape-urls` | Scrapear N URLs | Archivo .txt | JSON con resumen |
| `scrape-platform` | Scrapear N URLs misma plataforma | Plataforma + URLs | JSON con resumen |
| `show-summary` | Ver resumen de resultados | JSON previo | Terminal |
| `export-results` | Convertir formato JSON a CSV | JSON previo | CSV o JSON |

---

## Casos de Uso Comunes

### Caso 1: Scraping simple de una URL de Reddit
```bash
thinkscraper scrape-url https://reddit.com/r/Python
```

### Caso 2: Scraping de m√∫ltiples URLs con reporte
```bash
thinkscraper scrape-urls urls.txt --output results.json
thinkscraper show-summary results.json
thinkscraper export-results results.json report.csv --format csv
```

### Caso 3: Scraping por plataforma
```bash
thinkscraper scrape-platform stackoverflow \
  https://stackoverflow.com/q/1 \
  https://stackoverflow.com/q/2 \
  https://stackoverflow.com/q/3 \
  --output so_results.json
```

### Caso 4: Workflow completo
```bash
cat > urls.txt << EOF
https://reddit.com/r/python
https://stackoverflow.com/q/123
https://medium.com/@user/story
EOF

thinkscraper scrape-urls urls.txt --output results.json
thinkscraper show-summary results.json
thinkscraper export-results results.json export.csv --format csv
```

---

## Soluci√≥n de Problemas

### Error: "command not found: thinkscraper"
**Soluci√≥n:** Ejecuta:
```bash
uv pip install -e .
```

### Error: "No scraper supports URL"
**Soluci√≥n:** Verifica que la URL sea de una plataforma soportada. Usa `list-platforms` para ver las disponibles.

### Una URL falla y se detiene todo
**Soluci√≥n:** Usa la opci√≥n `--skip-errors`:
```bash
thinkscraper scrape-urls urls.txt --output resultados.json --skip-errors
```